InfoFlow AI

InfoFlow AI is a fullâ€‘stack, Retrievalâ€‘Augmented Generation (RAG) chatbot that answers questions based on your internal company documents. It uses a local LLM (Ollama) and local embeddings to ensure **complete data privacy** â€“ no information ever leaves your machine. The system is built with a modern React frontend and an Express.js backend, orchestrated with LangChain.

ğŸ”— **Repository:** [github.com/your-username/InfoFlow-ai](https://github.com/sohan-2005/InfoFlow-ai)  
ğŸŒ **Live Demo:** [Coming soon]

---

## âœ¨ Features

- **ğŸ’¬ Conversational AI** â€“ Ask natural language questions and receive precise, grounded answers.
- **ğŸ“„ Document Grounding** â€“ Answers are sourced from your own documents (PDF, TXT, MD) and displayed with citations.
- **ğŸ”’ 100% Private & Offline** â€“ Uses local HuggingFace embeddings and a local Ollama LLM â€“ no API keys, no data leaks.
- **ğŸŒ— Dark Mode** â€“ Toggle between light and dark themes seamlessly.
- **ğŸ“± Fully Responsive** â€“ Works beautifully on mobile, tablet, and desktop.
- **ğŸ‘¥ Team Page** â€“ Showcase the people behind the project.
- **ğŸ’¾ Persistent Chat** â€“ Conversation history is saved in `localStorage` and survives page reloads.
- **ğŸ§  RAG Pipeline** â€“ Built with LangChain: document loading, chunking, embedding, retrieval, and generation.

---

## ğŸ› ï¸ Tech Stack

| Frontend          | Backend           | AI / RAG                      |
|-------------------|-------------------|-------------------------------|
| React (Vite)      | Node.js + Express | LangChain.js                  |
| Tailwind CSS      | CORS, dotenv      | HuggingFace Embeddings (local)|
| React Router      | pdf-parse         | Ollama (llama3.2:3b)          |
| Framer Motion     |                   | MemoryVectorStore              |
| Lucide Icons      |                   |                                |

---

## ğŸ“‹ Prerequisites

Before you begin, ensure you have the following installed:

- [Node.js](https://nodejs.org/) (v18 or later)
- [npm](https://www.npmjs.com/) (usually comes with Node.js)
- [Ollama](https://ollama.com/) â€“ to run the local LLM
- Git (for cloning)

---

## ğŸš€ Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/your-username/InfoFlow-ai.git
cd InfoFlow-ai
```

### 2. Set up the backend
```bash
cd backend
npm install
```

- Create a `.env` file in the `backend` folder (optional, not required for local setup):
  ```
  PORT=5000
  ```
- Place your documents (`.pdf`, `.txt`, `.md`) inside the `backend/documents/` folder.  
  *A sample `company-policy.txt` is already included.*

- Start the backend server:
  ```bash
  npm run dev
  ```
  You should see:  
  `âœ… RAG ready` and `ğŸš€ Backend on port 5000`

### 3. Set up the frontend
Open a new terminal and navigate to the `frontend` folder:
```bash
cd frontend
npm install
npm run dev
```

The frontend will start at `http://localhost:5173`.

### 4. Start Ollama (if not already running)
Make sure Ollama is running in the background with the `llama3.2:3b` model pulled:
```bash
ollama pull llama3.2:3b
ollama serve
```

### 5. Use the app
- Open your browser to `http://localhost:5173`
- Navigate to the **Chat** page and ask questions about the documents you placed in `backend/documents/`.

---

## ğŸ—‚ï¸ Project Structure

```
InfoFlow-ai/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ documents/          # Your knowledge base (PDF, TXT, MD)
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â””â”€â”€ vectorStore.js  # RAG pipeline (embeddings, retrieval, chain)
â”‚   â”œâ”€â”€ server.js            # Express API
â”‚   â”œâ”€â”€ .env                 # Environment variables
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/      # Navbar, reusable UI
â”‚   â”‚   â”œâ”€â”€ pages/           # Home, Team, ChatPage
â”‚   â”‚   â”œâ”€â”€ context/         # ThemeContext, ChatContext
â”‚   â”‚   â”œâ”€â”€ lib/             # utils.js (cn helper)
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â””â”€â”€ index.css        # Tailwind imports
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”œâ”€â”€ postcss.config.js
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md
```

## ğŸ§ª How It Works (RAG Pipeline)

1. **Document Loading** â€“ PDFs and text files are read using `pdf-parse` and `fs`.
2. **Chunking** â€“ Documents are split into 1000â€‘character chunks with 200â€‘character overlap using `RecursiveCharacterTextSplitter`.
3. **Embedding** â€“ Each chunk is converted into a vector using the local `Xenova/all-MiniLM-L6-v2` model (HuggingFace).
4. **Storage** â€“ Vectors are stored in an inâ€‘memory `MemoryVectorStore`.
5. **Retrieval** â€“ When a user asks a question, the query is embedded and the topâ€‘3 most similar chunks are retrieved.
6. **Generation** â€“ The retrieved chunks are passed as context to the local LLM (`llama3.2:3b` via Ollama), which generates a naturalâ€‘language answer.
7. **Sources** â€“ The original document names and excerpts are returned alongside the answer for transparency.

---

## ğŸš§ Future Improvements

- [ ] User authentication (multiâ€‘tenant support)
- [ ] Dragâ€‘andâ€‘drop file upload in the UI
- [ ] Persistent vector database (e.g., Supabase pgvector) instead of inâ€‘memory
- [ ] Support for more document types (Word, Excel, etc.)
- [ ] Conversation memory across multiple turns
- [ ] Deployment to a free cloud service (Vercel + Render)

---

## ğŸ“„ License

This project is licensed under the MIT License â€“ see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgements

- [LangChain](https://www.langchain.com/) for the incredible RAG framework
- [Ollama](https://ollama.com/) for making local LLMs easy
- [Tailwind CSS](https://tailwindcss.com/) and [shadcn/ui](https://ui.shadcn.com/) for the beautiful components
- [Google Gemini](https://deepmind.google/technologies/gemini/) (explored during development)
